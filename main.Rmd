---
title: "Predicting Home Values in Philadelphia Using Spatial Regressions"
author: "Li, Jie; Wang, Yan; Zhang, Yihan"
date: "10/2/2022"
output:
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: hide
    theme: paper
    fig_caption: yes
  pdf_document: default
---

<link rel="stylesheet" href="css/styles.css">

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=FALSE, results="hide", message=FALSE, warning=FALSE)
```

```{r set-up}

# Several libraries and util functions from my GitHub
source("https://raw.githubusercontent.com/Leejere/r-setup/main/r_setup.r")

library(tmap)
```

# 1. Introduction

*Two paragraphs*

Home value prediction has been frequently explored in the field and industry of spatial statistics. In the previous assignment, we used the Ordinary Least Squares method to examine the relationship between median housing values and several neighborhood characteristics in Philadelphia using block-group-level data from the American Community Survey. The dependent variable was Median Housing Value by block group, and we explored predictors such as the percentage of residents with a bachelor's degree or higher, percentage of detached single-family housing, percentage of vacant units, and percentage of households under the poverty line.

Although the OLS model possessed reasonably high explaining power, it violated several assumptions required for an OLS model and faced limitations. Rather than being independent from each other, the observations tended to be spatially auto-correlated, and the model residuals were geographically nested rather than random. This meant that the *geographical location* itself was an important predictor of housing value, which was omitted in the OLS model. In this assignment, we built on top of the OLS model and used three *spatial* regression models, namely the *spatial lag*, *spatial error*, and the *geographically weighted* regression models.

# 2. Methods

## 2.1 Spatial autocorrelation

> Everything is related to everything else, but near things are more related than distant things.

Such a statement by Waldo Tobler in 1970 is the basic premise behind all spatial statistics. Conventional statistical modeling often requires that the observations are independent from each other. Geographical objects, however, are usually inherently correlated with their neighbors. For example, high-income families are more likely to live close to other high-income families, and nearby neighborhoods are more likely to have similar demographics and economics. This phenomenon is called *spatial autocorrelation*, and we need new statistical tools to handle spatially correlated data.

***Moran's I*** **is a measurement of global spatial autocorrelation.** The algorithm for Moran's I is written as follows. This index essentially measures the weighted mean of the co-variance of geographically *neighboring* observations against the *global* variance. This is, the higher the spatial autocorrelation, the higher the co-variances of neighboring observations, and thus the higher Moran's I. At the extreme, if neighboring observations have almost identical values, then the mean co-variance would almost become the variance, and the Moran's I reaches 1.

$$
Moran’s \ I=\frac{\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(X_i-\bar X)(X_j-\bar X)}{\sum_{i=1}^n\sum_{j=1}^nw_{ij}}}{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}}
$$

In the above equation, $\bar X$ is the global mean of the variable $X$, $X_i$ a value of $X$ at location $i$, and $X_j$ is the value of $X$ at another location $j$ in relation to $i$. $w_{ij}$ is a weight indexing location $i$ relative to $j$, and $n$ is the number of observations. In the nominator of the above equation, we calculated the weighted mean of of the covariance of each observed value $X_i$ with each of its neighbor $X_j$. The denominator is the global variance of $X$.

The weights of proximity are determined using different methods, e.g., by distance, or, for polygons, by weight matrices. The Rook matrix deems those polygons sharing a boundary segment as neighbors, whereas the others are non-neighbors. By the Queen matrix, on the other hand, a shared point will already suffice for neighbor-ship. Typically, statisticians would try different weight matrices to ensure that the results are not merely the artifact of the matrix being used.

**Testing the significance of Moran's I.** What value of Moran’s I indicates spatial autocorrelation? To test the significance of Moran's I, we need to do a hypothesis testing, where the null hypothesis is that no spatial autocorrelation exists, as opposed to the alternative hypothesis that there is significant spatial autocorrelation. First, we need to identify what the Moran's I would be if there were no spatial autocorrelation. To do that, we calculate a hypothetical Moran’s I after randomly "shuffling" the values attached to the geographies - an action attempting to erase any spatial autocorrelation that may exist. Once we do this enough times (e.g., 999 permutations), by plotting a histogram of the hypothetical Moran’s Is obtained, we can approximate the probability distribution of Moran’s I given no spatial autocorrelation, which is supposedly normal. Then, we can estimate the probability (p-value) of acquiring the actual Moran's I value were there no spatial autocorrelation. By convention, if the p-value is smaller than 0.05, then we reject the null hypothesis for the alternative hypothesis, stating that there is significant spatial autocorrelation.

**Local spatial autocorrelation.** As opposed the *global* Moran's I, the local index for spatial autocorrelation (LISA) measures the degree of spatial autocorrelation for each location. It measures how a particular value $X_i$ co-varies with its neighbors $X_j$ against the global variance. A greater positive value of LISA indicate the clustering of similar values near location $i$, whereas a negative value implies that location $i$ is an outlier in its milieu.

## 2.2 Review of OLS Regression and Assumptions

An Ordinary Least Square regression aims to construct a linear model that minimized the sum of squared residuals. This regression requires that the following assumptions hold true: Linear relationships between the dependent variable and the predictors, normal distribution and homoscedasticity of residuals, independence of observation and residuals, and non-collinearity between the predictors (if multiple regression).

In our case, the assumption of independent observations and residuals no longer holds due to spatial autocorrelation. We can test this assumption by examining the Moran’s I for the residuals after running an OLS model. Another way to test this assumption is to regress each residual on the average of nearby residuals (or "lagged residuals", "nearby" being defined by the Queen matrix). The coefficient of lagged residuals from this regression is known as $\lambda$ or $\rho$. It is also the slope rate of the residual-by-lagged-residual plot.

In the above process, a few other assumptions required in an OLS regression can be tested as well.

-   Homoscedasticity. We may use the Breusch-Pagan Test, the Koenker-Bassett Test, or the White Test, to test for homoscedasticity. The null hypothesis is that of homoscedasticity. If the p-value is smaller than 0.05, we reject the null hypothesis for the alternative hypothesis of heteroscedasticity.

-   Normality of residuals. We may use the Jarque-Bera Test to test for normality residual. Again, the null hypothesis is that of normality of residuals. If the p-value goes lower than 0.05, then we reject the null hypothesis for the alternative hypothesis of non-normality.

## 2.3 Spatial Lag and Spatial Error Regression

In this section, we present the models for the spatial lag and spatial error regressions.

**The spatial lag model** is a linear model that takes in the lagged $y$ (dependent variable) as an additional predictor. Here, the spatial lag model is built off of the model in the previous assignment and is written as follows:

$$
md\_housing\_value = \rho W_{md\_housing\_value}+\beta_0+\beta_1\cdot log\_n\_poverty+\beta_2\cdot pct\_bachelor+\beta_3\cdot pct\_single + \beta_4\cdot pct\_vacant+\epsilon
$$ 

where $W_{md\_housing\_value}$ is the lagged value of $md\_housing\_value$. For each observation, the lagged value is the weighted mean of the values at the neighboring locations. "Neighboring locations" are determined by a weights matrix. $\rho$ is the coefficient of this lagged variable. Holding all other variables constant, as $W_{md\_housing\_value}$ changes by one unit, the dependent variable will changes by $\rho$ units. Note that here $\rho$ is constrained between -1 and 1. This spatial lag model is estimated through Maximum Likelihood Estimation.

The other variables here are number of households under the poverty line (logged), percentage of residents of with a bachelor's degree or higher, percentage of housing units that are single-family detached, and the percentage of housing units that are vacant. $\beta_1$ to $\beta_4$ are the coefficients of these variables, and $\beta_0$ here is the intercept. Readers may refer to the first assignment for detailed interpretations of these terms. Finally, the $\epsilon$ here stands for the residual, or the difference between the predicted and observed values of the dependent variable.

**The spatial error regression** functions similarly to the spatial lag regression, except for that the term of lagged dependent variable is replaced by lagged residuals $W_\epsilon$. Running a spatial error regression requires a two-step process in theory:

-   Step 1: we run a regular OLS regression without the lagged term. Then, we calculate the residual of each observation.
-   Step 2: we determine how much of the residuals are "spatial". To do this, we regress the residuals on their nearest neighboring residuals and get $\lambda$. With this, we construct the spatial error model by adding the term of $\lambda W_\epsilon$. Conceptually, the spatial error model is written as follows:

$$
md\_housing\_value = \beta_0+\beta_1\cdot log\_n\_poverty+\beta_2\cdot pct\_bachelor+\beta_3\cdot pct\_single + \beta_4\cdot pct\_vacant+\epsilon
$$
$$
\epsilon = \lambda W\epsilon + u
$$

The first line is written similarly to an OLS model, where $\beta_0$ is the intercept, and $\beta_1$ to $\beta_4$ are the coefficients, and $\epsilon$ is the residual. In the second line, the residual is conceptually split into two parts, one of which is considered “spatial” ($\lambda W\epsilon$). Note that $\lambda$ here is restrained from -1 to 1. The spatial error model is also estimated through the Maximum Likelihood method.

It should be noted that the assumptions needed for the OLS model are still needed for both the spatial lag and spatial error models, except, of course, for the spatial independence of observations. The goal of the spatial lag and spatial error models is to address the possible spatial dependencies nested in the dataset. We expect that the residuals become no longer spatially auto-correlated and less heteroscedastic after taking the lagged terms into account.

After running the spatial lag or spatial error regressions, **model selection** should be performed, which determines whether the spatial lag or spatial error regressions perform better than the OLS regression that the former are based on. We may use the following criteria:

-   **Akaike Information Criterion or Schwarz Criterion**. These are measures of the goodness of fit of nested models (where one model is based on the other with a different component or complexity). By measuring the information that gets lost when a certain model is used, these indices describe the "trade-off" between the model's precision and complexity.

-   **Log likelihood** calculate the likelihood of the dataset as outcome, given a particular set of model parameters. The higher the likelihood, the more “likely” it is to have the dataset as it is, given the model held true, and therefore the better the model fits the data. As with the above item, the log likelihood method is also used only with nested models.

-   **Likelihood Ratio Test** compares the spatial model with the non-spatial OLS model. The null hypothesis is that the spatial model is not better than the non-spatial one. If the p-value is smaller than 0.05, then we may reject the null hypothesis for the alternative hypothesis that the spatial model is better than the non-spatial one given the data.

Another way to compare the models is by checking the Moran’s I of the regression residuals. The residuals from the spatial models are expected to have very low spatial depencency, which is an indicator that the spatial models fit the data better.

## 2.4 Geographically Weighted Regression

The **Simpson's Paradox** describes a situation where the modeled relationship between variables may not remain constant; that is, for a subset of observations, the relationship may be different from the one modeled from all the observations. Although the spatial lag and spatial error regression have addressed the problem with spatial dependencies, they still assume **spatial stationarity**, meaning the modeled relationship remains constant across space. It might be beneficial to have **different models** for different spatial locations, i.e., **local regressions**. This process of constructing local regressions for different locations in space is called the **geographically weighted regression (GWR)**.

The equation for the GWR model is written for each observation $i$:

$$

md\_housing\_value_i = \beta_{i0}+\beta_{i1}\cdot log\_n\_poverty_i+\beta_{i2}\cdot pct\_bachelor_i+\beta_{i3}\cdot pct\_single_i+\beta_{i4}\cdot pct\_vacant_i+\epsilon_i

$$

To get such an equation for each observation, we run a **local regression**, which takes the observations near location $i$ and run an OLS regression model. The observations entering into local regressions are weighted, and the weights are determined by **weighing function** or **kernel**. On the other hand, a local regression takes observations only within a certain **bandwidth** or distance, beyond which all other observations have a weight of zero.

There are two ways to determine the bandwidth.

-   **Fixed bandwidth.** A circle of radius $h$ is drawn around observation $i$. The location regression for location $i$ takes observations only within $h$ distance.

-   **Adaptive bandwidth.** A fixed number of nearest neighbors are selected for each local regression, and thus the bandwidth $h$ is adaptive.

Fixed bandwidth is more appropriate when the spatial distribution of the observations are even. Otherwise, the number of observations entering into each local regression may vary greatly. For the sparse places, too few observations enter the local regressions; whereas for the dense places, too many observations enter local regressions that they may no longer be "local" enough. In our case, as block group sizes vary across Philadelphia, we chose to use the adaptive bandwidth.

The assumptions required for a regular OLS regression are still required for the GWR model, including normality of residuals, homoscedasticity, and non-collinearity of predictors. The assumption for non-multicollinearity is worth noting here. As the GWR model builds a local regression for each location, we may run into issues when the values of one of the predictors cluster in space. In that case, this predictor will provide little explanation power in the clustering places. In the same way, we may run into issues when two or more predictors have similar patterns of clusters, which may produce unstable or unreliable results. In R, the **condition number** indicates when a local regression has issues with multicollinearity and therefore may have unreliable coefficients. Conventionally, we do not trust results for local regressions with a condition number greater than 30 or equal to null.

Unlike with OLS regressions, p-values are not part of the GWR output. This is a problem with multiple testing. If the p-value is $\alpha$, it means that under the null hypothesis, there is, although unlikely, still an $\alpha$ probability that we observe the result by pure chance. If $\alpha$ is reasonably small, then we can reject the null hypothesis. However, this rejection may turn out to be a type II error in which we deem something to be significant when it is actually not; after all, even if something is not significant, there is still an $\alpha$ probability of us rejecting the null hypothesis. The problem arises with multiple testing because as the number of tests go up, it becomes almost inevitable to make type II errors, and thus the basic premises of p-value reporting (rejecting the null hypothesis assuming type II errors are not likely) no longer holds. In the GWR model, each predictor in each local regression requires testing. Therefore, we do not usually report p-values in the GWR model.

# 3. Results

```{r import}

df <-  st_read("data/Regression Data.shp") %>%
  st_transform(crs) %>%
  dplyr::select(bg_id = AREAKEY, # Block Group ID
                md_housing_value = MEDHVAL, 
                md_housing_value_log = LNMEDHVAL,# Median Housing Value (owner-occupied)
                pct_bachelor = PCTBACHMOR, # Pct of residents w/ bachelor's +
                pct_vacant = PCTVACANT, # Pct of hs units that are vacant
                n_poverty = NBelPov100, # Number of hhs below poverty line
                md_hh_income = MEDHHINC, # Median household income
                pct_single = PCTSINGLES # Pct of hs units that are single-family detached
                ) %>% 
  mutate(n_poverty_log = log(n_poverty + 1),
         md_hh_income_log = log(md_hh_income + 1))

# Dictionary of variable names to their meaningful names
var_dict = c("bg_id" = "Block Group ID",
            "md_housing_value" = "Median Housing Value",
            "md_housing_value_log" = "Logged Median Housing Value",
            "pct_bachelor" = "Pct of Residents w/ Bachelor's Degree +",
            "pct_vacant" = "Pct of Housing Units Being Vacant",
            "n_poverty" = "# of Households under Poverty Line",
            "md_hh_income" = "Median Household Income",
            "pct_single" = "Pct of Housing Unites Single Family Detached",
            "n_poverty_log" = "Logged number of Households under Poverty Line",
            "md_hh_income_log" = "Logged Median Household Income")

```


## 3.1 Exploring Spatial Autocorrelation Through Global and Local Moran’s I

As a first step, we examined whether and how the dependent variable, median housing value (log-transformed), is spatially auto-correlated. We calculated the global Moran’s I for the dependent variable.

```{r queen-neighbors}

# create the queen weight dataframe
queen<-poly2nb(df, row.names=df$bg_id)

```

```{r queen-plots, echo=FALSE, eval=FALSE}

# plot continuous queen neighbors
plot(df %>% dplyr::select(bg_id), col='grey90', lwd=2)
xy<-coordinates(as_Spatial(df))
par(mfrow=c(1,1)) 
plot(queen, xy, col='red', lwd=10, add=TRUE)
title(main='Contiguous Queen Neighbors')
```

```{r queen-plots-2, echo=FALSE, eval=FALSE}

#see which region has only one neighbor
smallestnbcard<-card(queen) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen[[smallestnb[2]]]]<-'green'
fg[queen[[smallestnb[3]]]]<-'green'
fg[queen[[smallestnb[4]]]]<-'green'
plot(df %>% dplyr::select(bg_id), col=fg)
title(main='Regions with only 1 neighbor')

#see which region has most neighbors
largestnbcard<-card(queen)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen[[largestnb]]]<-'green'
plot(df %>% dplyr::select(bg_id), col=fg1)
title(main='Region with 27 neighbors')

```

```{r global-morans}

# Generalist the queen list for moran's I
queenlist<-nb2listw(queen, style = 'W')
# calculate Global Moran'I
morans_i_stats = moran(df$md_housing_value, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))
morans_i = morans_i_stats$I

```

Using a queen matrix, the global Moran’s I was calculated to be 0.68. But what does this tell us? By permutating the values across geometries, we plotted a histogram of what the Moran’s I would be should there be no spatial auto-correlation (see Fig. 1). The plot shows that the observed Moran’s I is much higher than what would be if there were no spatial auto-corrlation. Therefore, we are confident to state that the dependent variable is significantly spatially auto-correlated.

```{r morans-histogram, fig.cap="Fig. 1"}

# Then check to see whether the Moran’s I value is significant (using 999 permutations). Take a screenshot of your results to presentin your report (Moran’s I value for the sample, histogram of Moran’s I values forthe permutations, and the p-value that you obtain will need to be included).

moranMC<-moran.mc(df$md_housing_value, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations

# Plot the hypothetical Moran's Is under the 999 permutations and plot the observed moran's I
ggplot(as.data.frame(moranMC$res[c(1:999)]), aes(moranMC$res[c(1:999)])) +
  geom_histogram(binwidth = 0.002, fill = palette_hero_faded) +
  # Observed Moran's I
  geom_vline(aes(xintercept = morans_i), color = palette_hero, size = 1) +
  labs(x="Moran's I under 999 permutations", y="Count", title = "Permutated and Observed Moran's I") +
  annotate("text", x=0.58, y=10, label = "Observed\nMoran's I:\n" %>% paste0(morans_i %>% round(2)),
           hjust = 0) +
  plot_theme()

```

```{r moran-lag-plot}

#Create Moran plot (lagged value against observed value)
df$lag_housing_value = lag.listw(queenlist, df$md_housing_value)

ggplot(df, aes(x=md_housing_value, y=lag_housing_value)) +
  geom_point(color=palette_hero_faded, size=0.5) +
  geom_smooth(method = "lm", color=palette_hero) +
  labs(x="Median Housing Value", y="Lagged Median Housing Value (Queen)",
       title="Median Housing Value Is Highly Spatially Autocorrelated") +
  plot_theme()

```

Apart from the global Moran’s I, we also calculated the local Moran’s I (LISA) for each individual block group. A p-value is attached to each block group, indicating whether this block group's value is spatially clustered or not (Fig.2).

```{r local-morans, message=FALSE}

#Run local moran's I (LISA) 
LISA<-localmoran(df$md_housing_value, queenlist)

# Resulting an sf stating Local Moran's Is
df.LISA <-cbind(df, as.data.frame(LISA))

```

```{r local-morans-map, fig.cap="Fig.2"}

# map the p-value districution
df.LISA = df.LISA %>%
  mutate(p_value =
           case_when(Pr.z....E.Ii.. < 0.001 ~ "0.000 to 0.001",
                     Pr.z....E.Ii.. < 0.010 ~ "0.001 to 0.010",
                     Pr.z....E.Ii.. < 0.050 ~ "0.010 to 0.050",
                     TRUE ~ "0.050 to 1.000"),
         p_value = factor(p_value,
                          levels = c("0.000 to 0.001",
                                     "0.001 to 0.010",
                                     "0.010 to 0.050",
                                     "0.050 to 1.000")))

df.LISA %>%
  ggplot(aes(fill=p_value)) +
  geom_sf(color="#FFFFFF", size=0.2) +
  scale_fill_manual(values=c(palette_hero, palette_hero_faded, palette_water, "#dddddd"),
                    name = "P-Value") +
  map_theme()

```

The map reveals that housing values are most spatially auto-correlated in Center City, Northwest Philadelphia near Chestnut Hill, and the farthest northeast. In the below map, we coupled LISA with the value of the dependent variable. Any dependent variable higher than the citywide average is considered "High" and otherwise "Low"; any local Moran’s I higher than the global Moran’s I is considered "High" and otherwise "Low". Therefore, a "High-High" coupling means spatially clustered high values, and "Low-High" means well-clustered low values. On the other hand, "High-Low" means local high outliers, and "Low-Low" means local low outliers (see Fig.3). 

```{r loal-morans-categorised-map, fig.cap="Fig.3"}

# categorized p-value map
mean_value = mean(df$md_housing_value, na.rm=TRUE)
mean_lisa = mean(LISA[,1], na.rm=TRUE)

df.LISA = df.LISA %>%
  mutate(category =
           case_when(Pr.z....E.Ii.. > 0.05 ~ "Insignificant",
                     md_housing_value >= mean_value & Ii >= mean_lisa ~ "High-High",
                     md_housing_value >= mean_value & Ii < mean_lisa ~ "High-Low",
                     md_housing_value < mean_value & Ii >= mean_lisa ~ "Low-High",
                     TRUE ~ "Low-Low"),
         category = factor(category,
                           levels = c("High-High", "High-Low", "Low-High", "Low-Low", "Insignificant")))

df.LISA %>%
  ggplot(aes(fill = category)) +
  geom_sf(color="#FFFFFF", size=0.2) +
  scale_fill_manual(values = c(palette_primary, palette_primary_faded,
                               palette_hero, palette_hero_faded,
                               "#dddddd"),
                    name = "Categories") +
  map_theme()

```

## 3.2 OLS Regression and Its Assumptions: Review

In this section, we start by reviewing the basic OLS regression model. Readers may refer to the previous assignment for a more detailed recount.

### (a) The OLS model

```{r}
fit <- lm(md_housing_value_log ~ pct_vacant + pct_single 
          + pct_bachelor + n_poverty_log,
          data = df %>% st_drop_geometry())
summary(fit)
```

According to the model summary presented above, the OLS model explained around 66% of the variance in the dependent variable ($md_housing_value$) ($R^2$ is 0.6623 and Adjusted $R^2$ is 0.6615). The low p-value for F-test (p \< 0.0001) suggests that we can reject the null hypothesis that all coefficients in the model are 0.

All predictors used in this regression are highly significant ($p<0.0001$for all variables). The percentage of single house units($pct\_single$) and the percentage of individuals with bachelor's degree or higher($pct\_bachelor$) are positively associated with logged median household value, while the percentage of vacant houses($pct\_vacant$) and the logged number of households living in poverty($hh\_poverty\_log$) are negatively associated with logged median household value.

### (b) Examining heteroscedasticity and the normality of errors

The OLS model requires several assumptions to be met, notably the homoscedasticity of residuals and the normality of errors. Here, we examined the extend to which these assumptions are met.

**Tests on heteroscedasticity.** Here, we used the Breusch-Pagan Test, the Koenker-Bassett Test, and the White Test, to test for homo- or heteroscedasticity of the above OLS model.

```{r ols-bptest}

library(lmtest)
# Breusch-Pagan Test
bptest(fit, studentize=FALSE)

```

The Breusch-Pagan Test produced an output of 113.19, with a p-value very close to zero. Therefore, we rejected the null hypothesis of homoscedasticity for the alternative hypothesis of heteroscedasticity.

```{r ols-kbtest}

# Koenker-Bassett Test
bptest(fit, studentize=TRUE)

```

```{r ols-white-test, warning=FALSE}
library(whitestrap)
# White test
white_test(fit)

```

Likewise, the Koenker-Bassett Test and the White Test produced consistent results as the Breusch-Pagan Test, unanimously suggesting that strong heteroscedasticity was present in the OLS model.

**Normality of errors.** We used the Jarque-Bera test to test whether the errors of the model are normally distributed.

```{r ols-jarque-bera-test}

library(tseries)
jarque.bera.test(fit$residuals)

```

As the p-value goes down to near-zero, we rejected the null hypothesis of the normality of error for the alternative hypothesis of non-normality.

### (c) Spatial autocorrelation of residuals

For a more concrete understanding of the problem of spatial auto-correlation, we regressed the model residuals on the lagged residuals (residuals of queen neighbors). The below scatter plot (Fig.4) shows the standardized model residuals by the lag residuals, followed by a model summary of the regression of residuals. The “slope” of the below plot, or the coefficient of the regression, is 0.73, with a p-value close to zero. This confirms that the residuals of the OLS model are highly spatially nested.

```{r ols-residual-by-lag-plot, fig.cap="Fig.4"}

# First standardize the OLS regression residuals
res_standardized = rstandard(fit)

# Then get the lagged residuals
res_lag = lag.listw(queenlist, res_standardized)

# Plot
cbind(res_standardized, res_lag) %>%
  as.data.frame() %>%
  ggplot(aes(x=res_lag, y=res_standardized)) +
  geom_point(color=palette_hero_faded, size=1) +
  geom_smooth(method="lm", color=palette_hero) +
  labs(x="Lagged residuals", y="Standardized residuals") +
  plot_theme()

```
```{r}
res_fit = lm(res_standardized ~ res_lag)
summary(res_fit)
```

From the plot and the regression results, we identified strong spatial autocorrelation of the residuals. The coefficient of standardized residuals reached 0.73, and the p-value was near zero. 

### (d) Moran’s I for OLS residuals

Like we did in Section 3.1, we did 999 permutations of the OLS residuals to plot the below histogram of Moran’s Is, given no spatial auto-correlation. The observed Moran’s I for the residuals was much higher than the permutations, and we were confident to state that there was strong spatial auto-correlation of the OLS residuals, violating the assumptions required by an OLS model.

```{r residual-morans-histogram, fig.cap="Fig.5"}

moran_res = moran.mc(fit$residuals, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations

# Plot the hypothetical Moran's Is under the 999 permutations and plot the observed moran's I
ggplot(as.data.frame(moran_res$res[c(1:999)]), aes(moran_res$res[c(1:999)])) +
  geom_histogram(binwidth = 0.002, fill = palette_hero_faded) +
  # Observed Moran's I
  geom_vline(aes(xintercept = morans_i), color = palette_hero, size = 1) +
  labs(x="Moran's I under 999 permutations", y="Count", title = "Permutated and Observed Moran's I") +
  annotate("text", x=0.58, y=10, label = "Observed\nMoran's I:\n" %>% paste0(morans_i %>% round(2)),
           hjust = 0) +
  plot_theme()

```

## 3.3 Spatial Lag Regression

### (a) Model summary

A step further from the OLS model, next we used the spatial lag model, which takes the "lagged" dependent variable, or values of (queen) neighbor observations, into the equation and uses the Maximum Likehood Estimation method to estimate the coefficients. The summary of the spatial lag model is presented below.

```{r spatial-lag-model, warning=FALSE}

library(spatialreg)

lag_reg = lagsarlm(md_housing_value_log ~ pct_vacant + pct_single 
                   + pct_bachelor + n_poverty_log,
                   data = df %>% st_drop_geometry(),
                   queenlist)

summary(lag_reg)
```

The summary shows that the coefficient of the spatial term, $\rho$, is 0.65, and it is significant with a p-value very close to zero. This coefficient means that, holding all other variables constant, one unit of increase in the logged median housing values of queen-neighboring block groups is correlated with around 0.65 unit of increase in log median housing values.

Apart from the spatial lag term, the other terms in the model remain significant with p-values all smaller than 0.01. However, compared to the OLS model, all the non-spatial terms now have smaller coefficients and slightly higher p-values, meaning that, taking spatial auto-correlation into account, one unit of change in those predictors is correlated with a smaller change in median housing value. This indicates that the spatial lag term already explained a considerable portion of the variance of median housing value.

### (b) Tests on heteroscedasticity and the normality of errors

Let's perform the same tests on heteroscedasticity and the normality of errors as in Section 3.1. The Breusch-Pagan and Koenker-Bassett Tests below show that the spatial lag model still faces problems with heteroscedasticity, as the p-values are smaller than 0.05 for us to reject the null hypothesis of homoscedasticity.

```{r lag-model-bptest, warning=FALSE}
library(spatialreg)
bptest.Sarlm(lag_reg, studentize = FALSE)
```

```{r lag-model-kbtest, warning=FALSE}
bptest.Sarlm(lag_reg, studentize = TRUE)
```

### (c) Goodness-of-fit comparison with the OLS model

Now, let's compare the spatial lag model with the original OLS model in terms of goodness of fit. According the model summary above, the Akaike Information Criterion for the spatial lag model is 525.48, lower than the AIC for the OLS model, which is 1435. This indicates that the spatial lag model is a better fit for the data. In terms of log likelihood, the below summary shows that the OLS model produced a log likelihood of -711, where the spatial lag model had a log likelihood of -255. The likelihood ratio test produced a p-value close to zero, also indicating that the spatial lag model fits the data better.

```{r lag-model-log-likelihood-compare, warning=FALSE}

LR.Sarlm(lag_reg, fit)

```

### (d) Moran’s I scatterplot

Below, we produced a Moran’s I scatter plot of the residuals of the spatial lag model (residuals by lagged residuals of queen neighbors). The plot shows that the “slope” of the plot is negative and close to zero, meaning that the residuals are no longer spatially clustered, meaning much of the spatial auto-correlation is already accounted for in this model.

```{r lag-model-morans-scatterplot, fig.cap="Fig.6"}

residuals = lag_reg$residuals %>% scale(.)
residuals_lag = lag.listw(queenlist, residuals)

cbind(residuals, residuals_lag) %>%
  as.data.frame() %>%
  ggplot(aes(x=residuals_lag, y=residuals)) +
  geom_point(color=palette_hero_faded, size=1) +
  geom_smooth(method="lm", color=palette_hero) +
  labs(x="Lagged residuals", y="Standardized residuals") +
  plot_theme()

```

Based on all the criteria above, we were able to state that the spatial lag model performed better in fitting the data.

## 3.4 Spatial Error Regression



## 3.5 Geographically Weighted Regression (GWR)

```{r gwr_bandwidth}

library(spgwr)

#Setting an adaptive bandwidth
bw<-gwr.sel(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant, 
            data=as_Spatial(df),
            method = "aic",
            adapt = TRUE)

bw

# create a fixed bandwidth
bw_fixed<-gwr.sel(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant,  
            data=as_Spatial(df),
            method = "aic",
            adapt = FALSE)

bw_fixed

```

```{r gwr_model}

# gwr model with adpative bandwidth
gwrmodel<-gwr(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant,
              data= as_Spatial(df),
              adapt = bw, #adaptive bandwidth determined by proportion of observations accounted for
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel


# gwr model with fixed bandwidth
gwrmodel_fixed<-gwr(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant,
              data= as_Spatial(df),
              adapt = bw_fixed, #adaptive bandwidth determined by proportion of observations accounted for
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel_fixed

```

```{r gwr_adpative_summary}

summary(gwrmodel$SDF)

```

```{r gwr_mapping}

gwrresults<-as.data.frame(gwrmodel$SDF)

df <- df %>% 
  mutate(coef_n_poverty_log_st = gwrresults$n_poverty_log/gwrresults$n_poverty_log_se,
         coef_pct_vacant_st = gwrresults$pct_vacant / gwrresults$pct_vacant_se,
         coef_pct_bachelor_st = gwrresults$pct_bachelor / gwrresults$pct_bachelor_se,
         coef_pct_single_st = gwrresults$pct_single/gwrresults$pct_single_se,
         gwrE = gwrresults$gwr.e,
         localR2 = gwrresults$localR2)

# plotting
coef_n_poverty_log<-tm_shape(df)+
  tm_fill(col='coef_n_poverty_log_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of n_poverty_log', 
          palette ='-RdBu')+
  tm_layout(frame=FALSE, title = 'Number of Poverty (Log)')

coef_pct_vacant<-tm_shape(df)+
  tm_fill(col='coef_pct_vacant_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of pct_vacant', 
          palette='-RdBu')+
  tm_layout(frame=FALSE, title = 'Percentage of Housing Vacant')

coef_pct_bachelor<-tm_shape(df)+
  tm_fill(col='coef_pct_bachelor_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of pct_bachelor', 
          palette ='-RdBu')+
  tm_layout(frame=FALSE, title = 'Percentage of bachelor degree or more')

coef_pct_single<-tm_shape(df)+
  tm_fill(col='coef_pct_single_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of pct_single', 
          palette ='-RdBu')+
  tm_layout(frame=FALSE, title = 'Percentage of single family housing')

plot_list <-  list(coef_n_poverty_log, coef_pct_bachelor, coef_pct_single, coef_pct_vacant)

tmap_arrange(plot_list,  ncol=2)

```

```{r gwr_ggplot}

# All the variables for the maps
choropleth_var_list = c("coef_n_poverty_log_st", "coef_pct_vacant_st", "coef_pct_single_st",
                        "coef_pct_bachelor_st")

# # Create a list of the real names
# choropleth_name_list = c()
# for(var in choropleth_var_list){
#   if(substring(var, nchar(var) - 6, nchar(var)) == "_log_st"){
#     choropleth_name_list = c(choropleth_name_list, 
#                              paste0("Log ",
#                                     var_dict[substring(var, 1, nchar(var) - 4)]))
#   } else {
#     choropleth_name_list = c(choropleth_name_list, 
#                              paste0(var_dict[var]))
#   }
# }

choropleth_name_list <- c()
for (var in choropleth_var_list) {
  var_short <- gsub("coef_", "", var)
  var_short <- gsub("_st","", var_short)
  choropleth_name_list = c(choropleth_name_list, paste0(var_dict[var_short]))
}

# Initialize map list
map_list <-  list()

# Make a long table
df_long <-  df %>%
  dplyr::select(bg_id, choropleth_var_list) %>%
  gather("Legend", "Value", -bg_id, -geometry)

# Make the plots
for(i in seq(1,length(choropleth_var_list))){
  var = choropleth_var_list[i]
  this_sf = df_long %>% filter(., Legend == var)
  map_list[[var]] = 
    ggplot(this_sf) +
    geom_sf(aes(fill = Value), color = NA) +
    scale_fill_continuous(type = "viridis",
                      #labels = to_jenks_labels(this_df$Value, 5),
                      breaks = c(-6, -4, -2, 0, 2, 4, 6),
                      name = "") +
    labs(title = choropleth_name_list[i]) +
    map_theme(title_size = 9, tick_size = 7) +
    theme(legend.position = c(0.8, 0.25))
}

do.call(grid.arrange, c(map_list, ncol = 2, top = "Choropleth Maps"))

```

# 4. Discussion
