---
title: "Predicting Home Values in Philadelphia Using Spatial Regressions"
author: "Li, Jie; Wang, Yan; Zhang, Yihan"
date: "10/2/2022"
output:
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: hide
    theme: journal
    fig_caption: yes
  pdf_document: default
monofont: Hack
editor_options: 
  markdown: 
    wrap: sentence
---

<link rel="stylesheet" href="css/styles.css">

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=FALSE, results="hide", message=FALSE, warning=FALSE)
```

```{r set-up}

# Several libraries and util functions from my GitHub
source("https://raw.githubusercontent.com/Leejere/r-setup/main/r_setup.r")

library(tmap)
library(ggpmisc)
```

# 1. Introduction

Home value prediction has been frequently explored in the field and industry of spatial statistics. In the previous assignment, we used the Ordinary Least Squares method to examine the relationship between median housing values and several neighborhood characteristics in Philadelphia using block-group-level data from the American Community Survey. The dependent variable was Median Housing Value by block group, and we explored predictors such as the percentage of residents with a bachelor's degree or higher, percentage of detached single-family housing, percentage of vacant units, and percentage of households under the poverty line.

Although the OLS model possessed reasonably high explaining power, it violated several assumptions required of an OLS model and faced limitations. Rather than being independent from each other, the observations tended to be spatially auto-correlated, and the model residuals were geographically nested rather than random. This meant that the *geographical location* itself was an important predictor of housing value, which was omitted in the OLS model. In this assignment, we built on top of the OLS model and used three *spatial* regression models, namely the *spatial lag*, *spatial error*, and the *geographically weighted* regression models.

# 2. Methods

## 2.1 Spatial autocorrelation

> Everything is related to everything else, but near things are more related than distant things.

Such a statement by Waldo Tobler in 1970 is the basic premise behind all spatial statistics. Conventional statistical modeling often requires that the observations are independent from each other. Geographical objects, however, are usually inherently correlated with their neighbors. For example, high-income families are more likely to live close to other high-income families, and nearby neighborhoods are more likely to have similar demographics and economics. This phenomenon is called *spatial autocorrelation*, and we need new statistical tools to handle spatially correlated data.

***Moran's I*** **is a measurement of global spatial autocorrelation.** The algorithm for Moran's I is written as follows. This index essentially measures the weighted mean of the co-variance of geographically *neighboring* observations against the *global* variance. This is, the higher the spatial autocorrelation, the higher the co-variances of neighboring observations, and thus the higher Moran's I. At the extreme, if neighboring observations have almost identical values, then the mean co-variance would almost become the variance, and the Moran's I reaches 1.

$$
Moran’s \ I=\frac{\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(X_i-\bar X)(X_j-\bar X)}{\sum_{i=1}^n\sum_{j=1}^nw_{ij}}}{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}}
$$

In the above equation, $\bar X$ is the global mean of the variable $X$, $X_i$ a value of $X$ at location $i$, and $X_j$ is the value of $X$ at another location $j$ in relation to $i$. $w_{ij}$ is a weight indexing location $i$ relative to $j$, and $n$ is the number of observations. In the nominator of the above equation, we calculated the weighted mean of of the covariance of each observed value $X_i$ with each of its neighbor $X_j$. The denominator is the global variance of $X$.

The weights of proximity are determined using different methods, e.g., by distance, or, for polygons, by weight matrices. The Rook matrix deems those polygons sharing a boundary segment as neighbors, whereas the others are non-neighbors. By the Queen matrix, on the other hand, a shared point will already suffice for neighbor-ship. Typically, statisticians would try different weight matrices to ensure that the results are not merely the artifact of the matrix being used.

**Testing the significance of Moran's I.** What value of Moran's I indicates spatial autocorrelation? To test the significance of Moran's I, we need to do a hypothesis testing, where the null hypothesis is that no spatial autocorrelation exists, as opposed to the alternative hypothesis that there is significant spatial autocorrelation. First, we need to identify what the Moran's I would be if there were no spatial autocorrelation. To do that, we calculate a hypothetical Moran's I after randomly "shuffling" the values attached to the geographies - an action attempting to erase any spatial autocorrelation that may exist. Once we do this enough times (e.g., 999 permutations), by plotting a histogram of the hypothetical Moran's Is obtained, we can approximate the probability distribution of Moran's I given no spatial autocorrelation, which is supposedly normal. Then, we can estimate the probability (p-value) of acquiring the actual Moran's I value were there no spatial autocorrelation. By convention, if the p-value is smaller than 0.05, then we reject the null hypothesis for the alternative hypothesis, stating that there is significant spatial autocorrelation.

**Local spatial autocorrelation.** As opposed the *global* Moran's I, the local index for spatial autocorrelation (LISA) measures the degree of spatial autocorrelation for each location. It measures how a particular value $X_i$ co-varies with its neighbors $X_j$ against the global variance. A greater positive value of LISA indicate the clustering of similar values near location $i$, whereas a negative value implies that location $i$ is an outlier in its milieu.

## 2.2 Review of OLS Regression and Assumptions

An Ordinary Least Square regression aims to construct a linear model that minimized the sum of squared residuals. This regression requires that the following assumptions hold true: 

-   Linear relationships between the dependent variable and the predictors;

-   normal distribution and homoscedasticity of residuals;

-   independence of observation and residuals, and

-   non-collinearity between the predictors (if multiple regression).

In our case, the assumption of independent observations and residuals no longer holds due to spatial autocorrelation. We can test this assumption by examining the Moran's I for the residuals after running an OLS model. Another way to test this assumption is to regress each residual on the average of nearby residuals (or "lagged residuals", "nearby" being defined by the Queen matrix). The coefficient of lagged residuals from this regression is known as $\lambda$ or $\rho$. It is also the slope rate of the residual-by-lagged-residual plot.

In the above process, a few other assumptions required in an OLS regression can be tested as well.

-   Homoscedasticity. We may use the Breusch-Pagan Test, the Koenker-Bassett Test, or the White Test, to test for homoscedasticity. The null hypothesis is that of homoscedasticity. If the p-value is smaller than 0.05, we reject the null hypothesis for the alternative hypothesis of heteroscedasticity.

-   Normality of residuals. We may use the Jarque-Bera Test to test for normality residual. Again, the null hypothesis is that of normality of residuals. If the p-value goes lower than 0.05, then we reject the null hypothesis for the alternative hypothesis of non-normality.

## 2.3 Spatial Lag and Spatial Error Regression

In this section, we present the models for the spatial lag and spatial error regressions.

The **spatial lag model** is a linear model that takes in the lagged $y$ (dependent variable) as an additional predictor. Here, the spatial lag model is built off of the model in the previous assignment and is written as follows:


$$
md\_housing\_value = \rho W_{md\_housing\_value}+\beta_0+\beta_1\cdot log\_n\_poverty+\beta_2\cdot pct\_bachelor+\beta_3\cdot pct\_single + \beta_4\cdot pct\_vacant+\epsilon
$$
<br />
where $W_{md\_housing\_value}$ is the lagged value of $md\_housing\_value$. For each observation, the lagged value is the weighted mean of the values at the neighboring locations. "Neighboring locations" are determined by a weights matrix (the queen matrix in our case). $\rho$ is the coefficient of this lagged variable. Holding all other variables constant, as $W_{md\_housing\_value}$ changes by one unit, the dependent variable will changes by $\rho$ units. Note that here $\rho$ is artificially constrained between -1 and 1. This spatial lag model is estimated through Maximum Likelihood Estimation.

The other variables here are number of households under the poverty line (logged), percentage of residents of with a bachelor's degree or higher, percentage of housing units that are single-family detached, and the percentage of housing units that are vacant. $\beta_1$ to $\beta_4$ are the coefficients of these variables, and $\beta_0$ here is the intercept. Readers may refer to the first assignment for detailed interpretations of these terms. Finally, the $\epsilon$ here stands for the residual, or the difference between the predicted and observed values of the dependent variable.

The **spatial error regression** functions similarly to the spatial lag regression, except for that the term of lagged dependent variable is replaced by lagged residuals $W_\epsilon$. Running a spatial error regression requires a two-step process in theory:

-   Step 1: we run a regular OLS regression without the lagged term. Then, we calculate the residual of each observation.

-   Step 2: we determine how much of the residuals are "spatial". To do this, we regress the residuals on their nearest neighboring residuals and get $\lambda$. 

With this, we construct the spatial error model by adding the term of $\lambda W_\epsilon$. Conceptually, the spatial error model is written as follows:


$$
md\_housing\_value = \beta_0+\beta_1\cdot log\_n\_poverty+\beta_2\cdot pct\_bachelor+\beta_3\cdot pct\_single + \beta_4\cdot pct\_vacant+\epsilon
$$


$$
\epsilon = \lambda W\epsilon + u
$$
<br />
The first line is written similarly to an OLS model, where $\beta_0$ is the intercept, and $\beta_1$ to $\beta_4$ are the coefficients, and $\epsilon$ is the residual. In the second line, the residual is conceptually split into two parts, one of which is considered "spatial" ($\lambda W\epsilon$). Note that $\lambda$ here is restrained from -1 to 1. The spatial error model is also estimated through the Maximum Likelihood method.

It should be noted that the assumptions needed for the OLS model are still needed for both the spatial lag and spatial error models, except, of course, for the spatial independence of observations. The goal of the spatial lag and spatial error models is to address the possible spatial dependencies nested in the dataset. We expect that the residuals become no longer spatially auto-correlated and less heteroscedastic after taking the lagged terms into account.

After running the spatial lag or spatial error regressions, **model selection** should be performed, which determines whether the spatial lag or spatial error regressions perform better than the OLS regression that the former are based on. We may use the following criteria:

-   **Akaike Information Criterion or Schwarz Criterion**. These are measures of the goodness of fit of nested models (where one model is based on the other with a different component or complexity). By measuring the information that gets lost when a certain model is used, these indices describe the "trade-off" between the model's precision and complexity.

-   **Log likelihood** calculate the likelihood of the dataset as outcome, given a particular set of model parameters. The higher the likelihood, the more "likely" it would be to have the dataset as it is, given the model held true, and therefore the better the model fits the data. As with the above item, the log likelihood method is also used only with nested models.

-   **Likelihood Ratio Test** compares the spatial model with the non-spatial OLS model. The null hypothesis is that the spatial model is not better than the non-spatial one. If the p-value is smaller than 0.05, then we may reject the null hypothesis for the alternative hypothesis that the spatial model is better than the non-spatial one given the data.

Another way to compare the models is by checking the Moran's I of the regression residuals. The residuals from the spatial models are expected to have very low spatial depencency, which is an indicator that the spatial models fit the data better.

## 2.4 Geographically Weighted Regression

The **Simpson's Paradox** describes a situation where the modeled relationship between variables may not remain constant; that is, for a subset of observations, the relationship may be different from the one modeled from all the observations. Although the spatial lag and spatial error regression have addressed the problem with spatial dependencies, they still assume **spatial stationarity**, meaning the modeled relationship remains constant across space. It might be beneficial to have **different models** for different spatial locations, i.e., **local regressions**. This process of constructing local regressions for different locations in space is called the **geographically weighted regression (GWR)**.

The equation for the GWR model is written as follows. All the variables in this equation are interpreted in the same way as in an OLS regression, except that there is a seperate model for each observation $i$:


$$
md\_housing\_value_i = \beta_{i0}+\beta_{i1}\cdot log\_n\_poverty_i+\beta_{i2}\cdot pct\_bachelor_i+\beta_{i3}\cdot pct\_single_i+\beta_{i4}\cdot pct\_vacant_i+\epsilon_i
$$


To get such an equation for each observation, we run a **local regression**, which takes the observations near location $i$ and run an OLS regression model. The observations entering into local regressions are weighted, and the weights are determined by **weighing function** or **kernel**. On the other hand, a local regression takes observations only within a certain **bandwidth** or distance, beyond which all other observations have a weight of zero.

There are two ways to determine the bandwidth.

-   **Fixed bandwidth.** A circle of radius $h$ is drawn around observation $i$. The location regression for location $i$ takes observations only within $h$ distance.

-   **Adaptive bandwidth.** A fixed number of nearest neighbors are selected for each local regression, and thus the bandwidth $h$ is adaptive.

Fixed bandwidth is more appropriate when the spatial distribution of the observations are even. Otherwise, the number of observations entering into each local regression may vary greatly. For the sparse places, too few observations enter the local regressions; whereas for the dense places, too many observations enter local regressions that they may no longer be "local" enough. In our case, as block group sizes vary across Philadelphia, we chose to use the adaptive bandwidth.

The assumptions required for a regular OLS regression are still required for the GWR model, including normality of residuals, homoscedasticity, and non-collinearity of predictors. The assumption for non-multicollinearity is especially worth noting here. As the GWR model builds a local regression for each location, we may run into issues when the values of one of the predictors cluster in space. In that case, this predictor will provide little explanation power in the clustering places. In the same way, we may run into issues when two or more predictors have similar patterns of clusters, which may produce unstable or unreliable results. In R, the **condition number** indicates when a local regression has issues with multicollinearity and therefore may have unreliable coefficients. Conventionally, we do not trust results for local regressions with a condition number greater than 30 or equal to null.

Unlike with OLS regressions, p-values are not part of the GWR output. This is a problem with *multiple testing*. Under statistical hypothesis testing, if we get a p-value of $\alpha$, it means that under the null hypothesis, there is, although unlikely, still an $\alpha$ probability to observe the “significant” result by pure chance if the result is in reality insignificant. But if $\alpha$ is reasonably small, then we feel confident enough reject the null hypothesis. However, this rejection may turn out to be a type II error in which we deem something to be significant when it is actually not. The problem arises with multiple testing because as the number of tests mounts, it becomes almost inevitable to make type II errors in a least some of the tests, and thus the basic premises of p-value reporting (rejecting the null hypothesis assuming type II errors are not likely) no longer holds. In the GWR model, each predictor in each local regression requires testing. Therefore, we do not usually report p-values in the GWR model.

# 3. Results

```{r import}

# Read in the data
df <-  st_read("data/Regression Data.shp") %>%
  st_transform(crs) %>%
  dplyr::select(bg_id = AREAKEY, # Block Group ID
                md_housing_value = MEDHVAL, 
                md_housing_value_log = LNMEDHVAL,# Median Housing Value (owner-occupied)
                pct_bachelor = PCTBACHMOR, # Pct of residents w/ bachelor's +
                pct_vacant = PCTVACANT, # Pct of hs units that are vacant
                n_poverty = NBelPov100, # Number of hhs below poverty line
                md_hh_income = MEDHHINC, # Median household income
                pct_single = PCTSINGLES # Pct of hs units that are single-family detached
                ) %>% 
  mutate(n_poverty_log = log(n_poverty + 1),
         md_hh_income_log = log(md_hh_income + 1))

# Dictionary of variable names to their meaningful names
var_dict = c("bg_id" = "Block Group ID",
            "md_housing_value" = "Median Housing Value",
            "md_housing_value_log" = "Logged Median Housing Value",
            "pct_bachelor" = "Pct of Residents w/ Bachelor's Degree +",
            "pct_vacant" = "Pct of Housing Units Being Vacant",
            "n_poverty" = "# of Households under Poverty Line",
            "md_hh_income" = "Median Household Income",
            "pct_single" = "Pct of Housing Unites Single Family Detached",
            "n_poverty_log" = "Logged number of Households under Poverty Line",
            "md_hh_income_log" = "Logged Median Household Income")

```

## 3.1 Exploring Spatial Autocorrelation Through Global and Local Moran's I

As a first step, we examined whether and how the dependent variable, median housing value (log-transformed), is spatially auto-correlated. We calculated the global Moran's I for the dependent variable.

```{r queen-neighbors}

# create the queen weight dataframe
queen<-poly2nb(df, row.names=df$bg_id)

```

```{r queen-plots, echo=FALSE, eval=FALSE}

# plot continuous queen neighbors
plot(df %>% dplyr::select(bg_id), col='grey90', lwd=2)
xy<-coordinates(as_Spatial(df))
par(mfrow=c(1,1)) 
plot(queen, xy, col='red', lwd=10, add=TRUE)
title(main='Contiguous Queen Neighbors')
```

```{r queen-plots-2, echo=FALSE, eval=FALSE}

#see which region has only one neighbor
smallestnbcard<-card(queen) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen[[smallestnb[2]]]]<-'green'
fg[queen[[smallestnb[3]]]]<-'green'
fg[queen[[smallestnb[4]]]]<-'green'
plot(df %>% dplyr::select(bg_id), col=fg)
title(main='Regions with only 1 neighbor')

#see which region has most neighbors
largestnbcard<-card(queen)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen[[largestnb]]]<-'green'
plot(df %>% dplyr::select(bg_id), col=fg1)
title(main='Region with 27 neighbors')

```

```{r global-morans}

# Generalist the queen list for moran's I
queenlist<-nb2listw(queen, style = 'W')
# calculate Global Moran'I
morans_i_stats = moran(df$md_housing_value, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))
morans_i = morans_i_stats$I

print("Moran's I is " %>% paste0(morans_i %>% round(2)))
```

Using a queen matrix, the global Moran's I was calculated to be 0.68. But what does this tell us about the data set's spatial auto-correlation? To make sense of it, we wanted to see what the Moran's I *would be* if there were no spatial auto-correlation. By permuting values across geometries, we plotted a histogram of what the Moran's I would be should there be no spatial auto-correlation (see Fig. 1). The plot shows that the observed Moran's I is much higher than all the permuted scenarios. Therefore, we are confident to state that the dependent variable is significantly spatially auto-correlated.

```{r morans-histogram, fig.cap="Fig. 1. Permuted and observed Moran's I of the dependent variable", fig.align="center"}

# Then check to see whether the Moran’s I value is significant (using 999 permutations). Take a screenshot of your results to presentin your report (Moran’s I value for the sample, histogram of Moran’s I values forthe permutations, and the p-value that you obtain will need to be included).

moranMC<-moran.mc(df$md_housing_value, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations

# Plot the hypothetical Moran's Is under the 999 permutations and plot the observed moran's I
ggplot(as.data.frame(moranMC$res[c(1:999)]), aes(moranMC$res[c(1:999)])) +
  geom_histogram(binwidth = 0.002, fill = palette_hero_faded) +
  # Observed Moran's I
  geom_vline(aes(xintercept = morans_i), color = palette_hero, size = 1) +
  labs(x="Moran's I under 999 permutations", y="Count", title = "Permuted and Observed Moran's I") +
  annotate("text", x=0.58, y=10, label = "Observed\nMoran's I:\n" %>% paste0(morans_i %>% round(2)),
           hjust = 0) +
  plot_theme()

```

Fig. 2 is a Moran's I plot in which the relationship between the dependent variable and the lagged dependent variable is shown. The linear correlation is quite obvious, indicating high levels of spatial auto-correlation.

```{r moran-lag-plot, fig.cap="Fig. 2 Moran's I plot of the dependent variable", fig.align="center"}

# Create Moran plot (lagged value against observed value)
df$lag_housing_value = lag.listw(queenlist, df$md_housing_value)

ggplot(df, aes(x=md_housing_value, y=lag_housing_value)) +
  geom_point(color=palette_hero_faded, size=0.5) +
  geom_smooth(method = "lm", color=palette_hero) +
  labs(x="Median Housing Value", y="Lagged Median Housing Value (Queen)",
       title="Median Housing Value Is Highly Spatially Autocorrelated") +
  plot_theme()

```

Apart from the global Moran's I, we also calculated the local Moran's I (LISA) for each individual block group. A p-value is attached to each block group, indicating whether this block group's value is spatially clustered or not (Fig.2).

```{r local-morans, message=FALSE}

#Run local moran's I (LISA) 
LISA<-localmoran(df$md_housing_value, queenlist)

# Resulting an sf stating Local Moran's Is
df.LISA <-cbind(df, as.data.frame(LISA))

```

```{r local-morans-map, fig.cap="Fig. 3. Mapping of p-values of local Moran's I (LISA) of the dependent variable", fig.align="center"}

# map the p-value districution
df.LISA = df.LISA %>%
  mutate(p_value =
           case_when(Pr.z....E.Ii.. < 0.001 ~ "0.000 to 0.001",
                     Pr.z....E.Ii.. < 0.010 ~ "0.001 to 0.010",
                     Pr.z....E.Ii.. < 0.050 ~ "0.010 to 0.050",
                     TRUE ~ "0.050 to 1.000"),
         p_value = factor(p_value,
                          levels = c("0.000 to 0.001",
                                     "0.001 to 0.010",
                                     "0.010 to 0.050",
                                     "0.050 to 1.000")))

df.LISA %>%
  ggplot(aes(fill=p_value)) +
  geom_sf(color="#FFFFFF", size=0.2) +
  scale_fill_manual(values=c(palette_hero, palette_hero_faded, palette_water, "#dddddd"),
                    name = "P-Value") +
  labs(title = "P-values from Local Moran's I (LISA)") +
  map_theme()

```

The map reveals that housing values are most spatially auto-correlated in Center City, Northwest Philadelphia near Chestnut Hill, and the farthest northeast. 

In the below map (Fig. 4), we coupled LISA with the value of the dependent variable. Any dependent variable higher than the citywide average is considered "High" and otherwise "Low"; any local Moran's I higher than the global Moran's I is considered "High" and otherwise "Low". Therefore, a "High-High" coupling means spatially clustered high values, and "Low-High" means well-clustered low values. On the other hand, "High-Low" means local high outliers, and "Low-Low" means local low outliers (see Fig.3).

```{r loal-morans-categorised-map, fig.cap="Fig. 4. High/low housing value coupled with spatial clusteredness", fig.align="center"}

# categorized p-value map
mean_value = mean(df$md_housing_value, na.rm=TRUE)
mean_lisa = mean(LISA[,1], na.rm=TRUE)

df.LISA = df.LISA %>%
  mutate(category =
           case_when(Pr.z....E.Ii.. > 0.05 ~ "Insignificant",
                     md_housing_value >= mean_value & Ii >= mean_lisa ~ "High-High",
                     md_housing_value >= mean_value & Ii < mean_lisa ~ "High-Low",
                     md_housing_value < mean_value & Ii >= mean_lisa ~ "Low-High",
                     TRUE ~ "Low-Low"),
         category = factor(category,
                           levels = c("High-High", "High-Low", "Low-High", "Low-Low", "Insignificant")))

df.LISA %>%
  ggplot(aes(fill = category)) +
  geom_sf(color="#FFFFFF", size=0.2) +
  scale_fill_manual(values = c(palette_primary, palette_primary_faded,
                               palette_hero, palette_hero_faded,
                               "#dddddd"),
                    name = "Categories") +
  labs(title = "High/low housing value coupled with spatial clusteredness") +
  map_theme()

```

## 3.2 OLS Regression and Its Assumptions: Review

In this section, we start by reviewing the basic OLS regression model. Readers may refer to the previous assignment for a more detailed recount. The below output is a summary of the OLS model.

### (a) The OLS model

```{r ols-summary, results="markup"}
fit <- lm(md_housing_value_log ~ pct_vacant + pct_single 
          + pct_bachelor + n_poverty_log,
          data = df %>% st_drop_geometry())
summary(fit)
```

According to the model summary presented above, the OLS model explained around 66% of the variance in the dependent variable ($md_housing_value$) ($R^2$ is 0.6623 and Adjusted $R^2$ is 0.6615). The low p-value for F-test (p \< 0.0001) suggests that we can reject the null hypothesis that all coefficients in the model are 0.

All predictors used in this regression are highly significant ($p<0.0001$for all variables). The percentage of single house units($pct\_single$) and the percentage of individuals with bachelor's degree or higher($pct\_bachelor$) are positively associated with logged median household value, while the percentage of vacant houses($pct\_vacant$) and the logged number of households living in poverty($hh\_poverty\_log$) are negatively associated with logged median household value.

### (b) Examining heteroscedasticity and the normality of errors

The OLS model requires several assumptions to be met, notably the homoscedasticity of residuals and the normality of errors. Here, we examined the extend to which these assumptions are met.

**Tests on heteroscedasticity.** Here, we used the Breusch-Pagan Test, the Koenker-Bassett Test, and the White Test, to test for homo- or heteroscedasticity of the above OLS model. Their results are presented as code-chunk outputs as follows.

```{r ols-bptest, warning = FALSE}

library(lmtest)
# Breusch-Pagan Test
bptest(fit, studentize=FALSE)

```

The summary for the Breusch-Pagan Test is presented above. The output was 113.19, with a p-value very close to zero. Therefore, we rejected the null hypothesis of homoscedasticity for the alternative hypothesis of *heteroscedasticity*.

```{r ols-kbtest, results="markup"}

# Koenker-Bassett Test
bptest(fit, studentize=TRUE)

```

```{r ols-white-test, warning=FALSE, results="markup"}
library(whitestrap)
# White test
white_test(fit)

```

The above two summaries are the results of the Koenker-Bassett Test and the White Test. Like the Breusch-Pagan Test, these two tests produced consistent results, unanimously suggesting that strong heteroscedasticity was present in the OLS model.

**Normality of errors.** We used the Jarque-Bera test to test whether the errors of the model are normally distributed.

```{r ols-jarque-bera-test, results="markup"}

library(tseries)
jarque.bera.test(fit$residuals)

```

As the p-value goes down to near-zero, we rejected the null hypothesis of the normality of error for the alternative hypothesis of non-normality.

### (c) Spatial autocorrelation of residuals

For a more concrete understanding of the problem of spatial auto-correlation, we regressed the model residuals on the lagged residuals (residuals of queen neighbors). The below scatter plot (Fig.5) shows the standardized model residuals by the lag residuals, followed by a model summary of the regression of residuals. The "slope" of the below plot, or the coefficient of the regression, is 0.73, with a p-value close to zero. This confirms that the residuals of the OLS model are highly spatially nested.

```{r ols-residual-by-lag-plot, fig.cap="Fig. 5. Standardized residuals by lagged residuals in the OLS model", fig.align="center"}

# First standardize the OLS regression residuals
res_standardized = rstandard(fit)

# Then get the lagged residuals
res_lag = lag.listw(queenlist, res_standardized)

# Plot
cbind(res_standardized, res_lag) %>%
  as.data.frame() %>%
  ggplot(aes(x=res_lag, y=res_standardized)) +
  geom_point(color=palette_hero_faded, size=1) +
  geom_smooth(method="lm", color=palette_hero) +
  labs(x="Lagged residuals", y="Standardized residuals") +
  stat_poly_eq(aes(label = paste(after_stat(eq.label),
                                 after_stat(rr.label), sep = "*\", \"*"))) +
  plot_theme()

```

From the plot and the regression results, we identified strong spatial autocorrelation of the residuals. The coefficient of standardized residuals reached 0.73, and the $R^2$ was associated with a p-value very close to zero.

### (d) Moran's I for OLS residuals

Like in Section 3.1, we did 999 permutations of the OLS residuals to plot the below histogram of Moran's Is, given no spatial auto-correlation. The observed Moran's I for the residuals was much higher than the permutations (Fig. 6), and we were confident to state that there was strong spatial auto-correlation of the OLS residuals, violating the assumptions required by an OLS model.

```{r residual-morans-histogram, fig.cap="Fig. 6. Permuted and observed Moran's I of the OLS model residuals", fig.align="center"}

moran_res = moran.mc(fit$residuals, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations

# Plot the hypothetical Moran's Is under the 999 permutations and plot the observed moran's I
ggplot(as.data.frame(moran_res$res[c(1:999)]), aes(moran_res$res[c(1:999)])) +
  geom_histogram(binwidth = 0.002, fill = palette_hero_faded) +
  # Observed Moran's I
  geom_vline(aes(xintercept = morans_i), color = palette_hero, size = 1) +
  labs(x="Moran's I under 999 permutations", y="Count", title = "Permuted and Observed Moran's I") +
  annotate("text", x=0.58, y=10, label = "Observed\nMoran's I:\n" %>% paste0(morans_i %>% round(2)),
           hjust = 0) +
  plot_theme()

```

## 3.3 Spatial Lag Regression

### (a) Model summary

A step further from the OLS model, next we used the spatial lag model, which takes the "lagged" dependent variable, or values of (queen) neighbor observations, into the equation and uses the Maximum Likelihood Estimation method to estimate the coefficients. The summary of the spatial lag model is presented below.

```{r spatial-lag-model, warning=FALSE, results="markup"}

library(spatialreg)

lag_reg = lagsarlm(md_housing_value_log ~ pct_vacant + pct_single 
                   + pct_bachelor + n_poverty_log,
                   data = df %>% st_drop_geometry(),
                   queenlist)

summary(lag_reg)
```

The summary shows that the overall goodness of fit improved significantly, as the AIC went down from 1,435 to 525. The coefficient of the spatial term, $\rho$, is 0.65, and it is significant with a p-value very close to zero. This coefficient means that, holding all other variables constant, one unit of increase in the logged median housing values of queen-neighboring block groups is correlated with around 0.65 unit of increase in log median housing values.

Apart from the spatial lag term, the other terms in the model remain significant with p-values all smaller than 0.01. However, compared to the OLS model, all the non-spatial terms now have smaller coefficients and slightly higher p-values, meaning that, taking spatial auto-correlation into account, one unit of change in those predictors is correlated with a smaller change in median housing value. This indicates that the spatial lag term already explained a considerable portion of the variance of median housing value.

### (b) Tests on heteroscedasticity and the normality of errors

Let's perform the same tests on heteroscedasticity and the normality of errors as in Section 3.1. The Breusch-Pagan and Koenker-Bassett Tests summaries are shown below. The spatial lag model still faced problems with heteroscedasticity, as the p-values are smaller than 0.05 for us to reject the null hypothesis of homoscedasticity.

```{r lag-model-bptest, warning=FALSE, results="markup"}

bptest.Sarlm(lag_reg, studentize = FALSE)
```

```{r lag-model-kbtest, warning=FALSE, results="markup"}
bptest.Sarlm(lag_reg, studentize = TRUE)
```

### (c) Goodness-of-fit comparison with the OLS model

Now, let's compare the spatial lag model with the original OLS model in terms of goodness of fit. According the model summary above, the Akaike Information Criterion for the spatial lag model was 525.48, lower than the AIC for the OLS model, which was 1,435. This indicated that the spatial lag model is a better fit for the data. In terms of log likelihood, the below summary shows that the OLS model produced a log likelihood of -711, where the spatial lag model had a log likelihood of -255. The likelihood ratio test produced a p-value close to zero, also indicating that the spatial lag model fits the data better.

```{r lag-model-log-likelihood-compare, warning=FALSE, results="markup"}

LR.Sarlm(lag_reg, fit)

```

### (d) Moran's I scatterplot

Below, we produced a Moran's I scatter plot of the residuals of the spatial lag model (residuals by lagged residuals of queen neighbors). The plot shows that the "slope" of the plot is negative and close to zero, meaning that the residuals are no longer spatially clustered, meaning much of the spatial auto-correlation is already accounted for in this model.

```{r lag-model-morans-scatterplot, fig.cap="Fig. 7. Standardized residuals by lagged residuals in the spatial lag model", fig.align='center'}

residuals = lag_reg$residuals %>% scale(.)
residuals_lag = lag.listw(queenlist, residuals)

cbind(residuals, residuals_lag) %>%
  as.data.frame() %>%
  ggplot(aes(x=residuals_lag, y=residuals)) +
  geom_point(color=palette_hero_faded, size=1) +
  geom_smooth(method="lm", color=palette_hero) +
  stat_poly_eq(aes(label = paste(after_stat(eq.label),
                                 after_stat(rr.label), sep = "*\", \"*"))) +
  labs(x="Lagged residuals", y="Standardized residuals",
       title = "Lagged residuals by standardized residuals in the spatial lag model") +
  plot_theme()

```

Based on all the criteria above, we were able to state that the spatial lag model performed better in fitting the data.

## 3.4 Spatial Error Regression

### (a) Model summary

Similarly, the below code-chunk output presents the model summary of the spatial error model.

```{r spatial-error-model, warning=FALSE, results="markup"}

error_reg = errorsarlm(md_housing_value_log ~ pct_vacant + pct_single 
                   + pct_bachelor + n_poverty_log,
                   data = df %>% st_drop_geometry(),
                   queenlist)

summary(error_reg)
```

The overall goodness of fit is better than the OLS model, as the AIC went down from 1,435 to 759. The coefficient of the spatial error term, $\lambda$, is 0.81, and it is significant with a p-value very close to zero. This coefficient means that, under the OLS regression, about 0.81 of the residual term can be attributed to spatial auto-correlation.

Apart from the spatial lag term, the other terms in the model remain significant with p-values all smaller than 0.01. However, compared to the OLS model, all the non-spatial terms now have smaller coefficients and slightly higher p-values, meaning that, taking spatial auto-correlation into account, one unit of change in those predictors is correlated with a smaller change in median housing value. This indicates that the spatial lag term already explained a considerable portion of the variance of median housing value.

### (b) Tests on heteroscedasticity and the normality of errors

Again, we performed the same tests on heteroscedasticity and the normality of errors as in Section 3.1. The Breusch-Pagan and Koenker-Bassett Tests summaries are shown below. The spatial error model still faced some problems with heteroscedasticity, as the Breusch-Pagan Test still produced a p-value lower than 0.05. However, the Koenker-Bassett Test result was no longer significant, indicating that the problem of heteroscedasticity was very much relieved in the spatial error model.

```{r error-model-bptest, warning=FALSE, results="markup"}

bptest.Sarlm(error_reg, studentize = FALSE)
```

```{r error-model-kbtest, warning=FALSE, results="markup"}
bptest.Sarlm(error_reg, studentize = TRUE)
```

### (c) Goodness-of-fit comparison between OLS, spatial lag, and spatial error models

Now, let's compare the spatial error model with the original OLS model in terms of goodness of fit. According the model summary above, the Akaike Information Criterion for the spatial error model was 759, lower than the AIC for the OLS model, which was 1,435., but higher than the spatial lag model. This indicated that the spatial error model is a better fit for the data compared to the OLS model, but not necessarily better than the spatial lag model. 

In terms of log likelihood, the below summary shows that the OLS model produced a log likelihood of -711, where the spatial error model had a log likelihood of -373. The likelihood ratio test produced a p-value close to zero, also indicating that the spatial error model fits the data better compared to the OLS model. However, the log likelihood of the spatial error model was still worse than the spatial lag model.

```{r error-model-log-likelihood-compare, warning=FALSE, results="markup"}

LR.Sarlm(error_reg, fit)

```

### (d) Moran's I scatterplot

Below, we produced a Moran's I scatter plot of the residuals of the spatial error model (residuals by lagged residuals of queen neighbors). The plot shows that the "slope" of the plot is negative and close to zero, meaning that the residuals are no longer spatially clustered, meaning much of the spatial auto-correlation is already accounted for in this model.

```{r error-model-morans-scatterplot, fig.cap="Fig. 8. Standardized residuals by lagged residuals in the spatial error model", fig.align="center"}

residuals = error_reg$residuals %>% scale(.)
residuals_lag = lag.listw(queenlist, residuals)

cbind(residuals, residuals_lag) %>%
  as.data.frame() %>%
  ggplot(aes(x=residuals_lag, y=residuals)) +
  geom_point(color=palette_hero_faded, size=1) +
  geom_smooth(method="lm", color=palette_hero) +
  stat_poly_eq(aes(label = paste(after_stat(eq.label),
                                 after_stat(rr.label), sep = "*\", \"*"))) +
  labs(x="Lagged residuals", y="Standardized residuals",
       title = "Lagged residuals by standardized residuals in the spatial error model") +
  plot_theme()

```

## 3.5 Geographically Weighted Regression (GWR)

### (a) Bandwidth Selection

The first step to run Geographically Weighted Regression is to determine the bandwidth. As elaborated in the Methods section, the selection of bandwidth can be **Fixed** (i.e., the location regression for location $i$ takes observations within $h$ distance), and **Adaptive** (i.e., a fixed number of nearest neighbors are selected for each local regression, and the distance/bandwidth $h$ is adaptive). The selection process below compared the AICc's (corrected AIC's) between different bandwidths and different numbers of neighbors selected. The optimized bandwidth or number of neighbors is selected based on the optimized AICc, which should be the smallest within the tested group.


```{r gwr-bandwidth-fixed, eval=FALSE}

library(spgwr)

# create a fixed bandwidth
bw_fixed<-gwr.sel(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant,  
            data=as_Spatial(df),
            method = "aic",
            adapt = FALSE)
```

```{r echo=FALSE}
bw_fixed = 2863.492
```

```{r gwr-bandwidth-fixed-show, results="markup"}
print("The fixed bandwidth is " %>% paste0(bw_fixed %>% round(3)))
```

```{r gwr-bandwidth-adaptive, eval=FALSE}

#Setting an adaptive bandwidth
bw<-gwr.sel(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant, 
            data=as_Spatial(df),
            method = "aic",
            adapt = TRUE)
```

```{r echo=FALSE}
bw = 0.00813
```

```{r gwr-bandwidth-adaptive-show, results="markup"}
print("The adaptive bandwidth should capture the proportion of " %>% 
        paste0(bw %>% round(5)) %>% 
        paste0(" of all observations"))
```
For the Fixed Bandwidth method, the optimized bandwidth is 2863.5 ft. For the Adaptive Bandwidth Method, the bandwidth should capture about 13 nearest neighbors. This was calculated by multiplying the total number of observations by the proportion of observations.

### (b) Model Comparison

The model summaries of the GWR, first with fixed bandwidth and then with adaptive bandwidth, are presented below.



```{r gwr_model-fixed, warning=FALSE, results="markup"}

# gwr model with adpative bandwidth
gwrmodel <- spgwr::gwr(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant,
              data= as_Spatial(df),
              adapt = bw, #adaptive bandwidth determined by proportion of observations accounted for
              gweight=spgwr::gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel

```


```{r gwr-model-adaptive, warning=FALSE, results="markup"}

# gwr model with fixed bandwidth
gwrmodel_fixed <- spgwr::gwr(formula=md_housing_value_log~n_poverty_log + pct_bachelor +
              pct_single + pct_vacant,
              data=as_Spatial(df),
              bandwidth = bw_fixed, #fixed bandwidth
              gweight=spgwr::gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)

gwrmodel_fixed

```

A more detailed summary of coefficients of the GWR model with adaptive bandwidths is presented below.

```{r gwr-adpative-summary-sdf, results="markup"}

summary(gwrmodel$SDF)

```


In this section, we will use two metrics, global R-squared and Akaike Information Criteria(AIC) to compare the goodness of fit among regression models we explored in this report. 

-   **R-squared.** The OLS model successfully explained about 66% of the variance in the dependent variable ($md\_housing\_value$) ($R^2$ is 0.6623 and Adjusted $R^2$ is 0.6615). By comparison, the GWR model explained around 85% of the variance in the dependent variable (Quasi-global R2: 0.8479244), which is significantly better than the OLS model.

-   **Akaike Information Criteria (AIC).** Apart from the commonly used R-squared, AIC is a better measure to compare model performance by taking model complexity into account. When the AIC value of one model is more-than-3 smaller than the other, the model with smaller AIC value can be considered to be performing better. As displayed in the following table, the AIC value of GWR is significantly lower than the AIC's of all other three models, suggesting that GWR has the best performance.

|     | OLS | Spatial Lag | Spatial Error | GWR (Adaptive) | GWR(Fixed) |
|-----|-----|-------------|---------------|----------------|------------|
| AIC |1435 | 525.48      |  759.35 | 308.71         | 352.35     |


-   **Spatial autocorrelation in residuals.** The Moran's I scatter plot below showed the residuals of the GWR model and fitted it by the lagged residuals of queen neighbors. The coefficient of the fitted line is positive and very close to zero(0.00479), which means the residuals are not spatially clustered. Compared with the OLS ($\beta = 0.732$, $R^2 = 0.23$), spatial lag ($\beta = -0.423$, $R^2 = 0.03$) and spatial error model, the GWR model ($\beta = 0.0984$, $R^2 = 0.04$) has the closest coefficients to 0, suggesting that the GWR model did best in accounting for spatial autocorrelation.

```{r gwr_result}

gwrresults<-as.data.frame(gwrmodel$SDF)

df <- df %>% 
  mutate(coef_n_poverty_log_st = gwrresults$n_poverty_log/gwrresults$n_poverty_log_se,
         coef_pct_vacant_st = gwrresults$pct_vacant / gwrresults$pct_vacant_se,
         coef_pct_bachelor_st = gwrresults$pct_bachelor / gwrresults$pct_bachelor_se,
         coef_pct_single_st = gwrresults$pct_single/gwrresults$pct_single_se,
         gwrE = gwrresults$gwr.e,
         localR2 = gwrresults$localR2)

```

```{r gwr_moran_scatterplot, fig.cap="Fig. 9. Standardized residuals by lagged residuals in the GWR model", fig.align="center"}

#residuals = as.numeric(gwrresults$pred.se %>% scale(.))
moran.mc(residuals, queenlist, 999, alternative="two.sided")
#moran.plot(residuals, queenlist)

residuals <-  gwrresults$pred.se %>% scale(.)
residuals_gwr <-  lag.listw(queenlist, residuals)


cbind(residuals, residuals_gwr) %>%
  as.data.frame() %>%
  ggplot(aes(x=residuals, y=residuals_gwr)) +
  geom_point(color=palette_hero_faded, size=1) +
  geom_smooth(method="lm", color=palette_hero) +
  labs(x="Residuals", y="Spatially Lagged Residuals") +
  stat_poly_eq(aes(label = paste(after_stat(eq.label),
                                 after_stat(rr.label), sep = "*\", \"*"))) +
  plot_theme()

```

### (c) Local Regressions

**Local R-squared** measures the goodness of fit of the model at each regression point. The Local R-squared map below shows where the model predicts well (high Local R-squared) or poorly (low local R-squared). Referring back to the spatial distribution maps plotted in Assignment 1, there is no obvious spatial similarities by observation. The clustering of low local R-squared values locates in North Philadelphia, Manayunk and Parkside. This sptial pattern provided us the hint to explore other valuable variables that might be missing in the regression model.

```{r gwr_local_rsquare, fig.cap="Fig. 10 Mapping of local R-Squared in the GWR model", fig.align="center"}

ggplot(df) +
    geom_sf(aes(fill = localR2), color = NA) +
    scale_fill_continuous(type = "viridis",
                      #labels = to_jenks_labels(this_df$Value, 5),
                      breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9,1),
                      name = "") +
    labs(title = "Distribution of local R-Squared") +
    map_theme(title_size = 9, tick_size = 7) +
    theme(legend.position = c(0.8, 0.25))

```

The following maps plotted the **standardized coefficients ($\beta/SE$)** for each independent variable. When the absolute value of the ratio between coefficient and standard error is no less than 2 ($|\beta/SE|\geq 2$), we can assume the relationship between the dependent and independent variable is significant.

-   For variable Logged number of poverty, the relationships with the dependent variable are likely not significant for most of the regression points (light blue and light red). However, there are possibly significant negative relationship exist in north east Philadelphia, Navy Yard and east of Strawberry Mansion.

-   For variable Percentage of bachelor degree and more, there are plausible significant positive relationships within the dark red areas. The light red areas, on the other hand, are likely to be lacking significant positive relationships. 

-   For variable Percentage of single family housing, there are possibly significant positive relationships in the darker red areas in northern parts of Philedalphia, and a possibly significant negative relationship in darker blue areas in southern parts of the city. 

-   For variable Percentage of housing vacant, a negative relationship with the dependent variable is likely to be significant in the darker blue areas. It is also noticeable that the area within northern center city shows a possibly significant positive relationship with the dependent variable. 

```{r gwr_mapping, fig.cap="Fig. 11. Mapping of coefficients from the GWR model", fig.align="center"}

# plotting
coef_n_poverty_log<-tm_shape(df)+
  tm_fill(col='coef_n_poverty_log_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of n_poverty_log', 
          palette ='-RdBu')+
  tm_layout(frame=FALSE, title = 'Number of Poverty (Log)',
            legend.text.size = 0.3,
            legend.title.size = 0.5,
            title.size = 0.5)

coef_pct_vacant<-tm_shape(df)+
  tm_fill(col='coef_pct_vacant_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of pct_vacant', 
          palette='-RdBu')+
  tm_layout(frame=FALSE, title = 'Percentage of Housing Vacant',
            legend.text.size = 0.3,
            legend.title.size = 0.5,
            title.size = 0.5)

coef_pct_bachelor<-tm_shape(df)+
  tm_fill(col='coef_pct_bachelor_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of pct_bachelor', 
          palette ='-RdBu')+
  tm_layout(frame=FALSE, title = 'Percentage of bachelor degree or more',
            legend.text.size = 0.3,
            legend.title.size = 0.5,
            title.size = 0.5)

coef_pct_single<-tm_shape(df)+
  tm_fill(col='coef_pct_single_st', breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), title='Standardized coefficient of pct_single', 
          palette ='-RdBu')+
  tm_layout(frame=FALSE, title = 'Percentage of single family housing',
            legend.text.size = 0.3,
            legend.title.size = 0.5,
            title.size = 0.5)

plot_list <-  list(coef_n_poverty_log, coef_pct_bachelor, coef_pct_single, coef_pct_vacant)

tmap_arrange(plot_list,  ncol=2)

```

# 4. Discussion

In this assignment, we addressed one particular problem that the previous OLS model faced: the violation of the assumption of the independence of observations, namely, spatial auto-correlation. We built on top of the OLS model and used three *spatial* regression models, namely the *spatial lag*, *spatial error*, and the *geographically weighted* regression models. 

The preliminary explorations indicated that there was strong spatial auto-correlation in the distribution of housing values. Compared to the OLS models, the spatial models performed better by taking the spatial nestedness of housing values into account. The residuals of the spatial models no longer exhibited significant spatial auto-correlation, and the problems of heteroscedasticity became much alleviated. In the spatial lag and spatial error models, the coefficients for the non-spatial predictors became smaller than in the OLS model, suggesting that location itself accounted for a considerable portion of the variance of housing values.

On the other hand, the GWR model addressed the problem of spatial non-stationarity, which the spatial lag and spatial error models failed to address. The GWR model was able to identify different extents to which the predictors were correlated with housing value in different parts of the city. Overall, the GWR model with adaptive bandwidths performed the best among all the spatial models, with the smallest AIC and largest R-Squared.

Despite performing better than the OLS model, the spatial model still faced limitations, as some model assumptions are still unmet. For example, in the spatial lag and spatial error models, the assumption of homoscedasticity was still violated, although the problem was less severe than in the OLS model. The spatial lag and spatial error models assumed spatial stationarity, which does not conform with the reality. Furthermore, multi-collinearity among the predictors were still a problem, especially for the GWR model.
